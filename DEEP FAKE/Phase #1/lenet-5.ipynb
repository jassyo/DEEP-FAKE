{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision.models import GoogLeNet, GoogLeNet_Weights\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the data\n",
    "# First Transformer applied to Lenet-5\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32,32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "# ])\n",
    "\n",
    "# Second Transformer applied to VGG16\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root='E:\\\\Temp\\\\test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fake': 0, 'Real': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Le_Net5(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Le_Net5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fcl1 = nn.Linear(16*5*5, 120)\n",
    "        self.fcl2 = nn.Linear(120, 84)\n",
    "        self.fcl3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = torch.relu(self.conv1(X))\n",
    "        X = torch.max_pool2d(X, 2)\n",
    "        X = torch.relu(self.conv2(X))\n",
    "        X = torch.max_pool2d(X, 2)\n",
    "        X = X.view(X.size(0), -1) # flatten the tensor\n",
    "        X = torch.relu(self.fcl1(X))\n",
    "        X = torch.relu(self.fcl2(X))\n",
    "        X = self.fcl3(X)\n",
    "        return X\n",
    "    \n",
    "\n",
    "model = Le_Net5(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1, Loss: 0.6245187259556955\n",
      "Fold 1, Epoch 2, Loss: 0.5604676243505979\n",
      "Fold 1, Epoch 3, Loss: 0.5304904164452302\n",
      "Fold 1, Epoch 4, Loss: 0.5034905452477304\n",
      "Fold 1, Epoch 5, Loss: 0.4841533240519072\n",
      "Fold 1, Epoch 6, Loss: 0.4531817810054411\n",
      "Fold 1, Epoch 7, Loss: 0.43091117263885964\n",
      "Fold 1, Epoch 8, Loss: 0.40682335067213626\n",
      "Fold 1, Epoch 9, Loss: 0.38768051146415244\n",
      "Fold 1, Epoch 10, Loss: 0.36950460376969557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Validation Precision: 0.00%\n",
      "Fold 1, Validation F1 Score: 0.00%\n",
      "Fold 1, Validatoin Confusion Matrix\n",
      " []\n",
      "Fold 1, Test Precision: 0.73%\n",
      "Fold 1, Test F1 Score 0.76%\n",
      "Fold 1, Test Confusion Matrix\n",
      " [[1340  524]\n",
      " [ 372 1399]]\n",
      "Fold 2, Epoch 1, Loss: 0.4281153584781446\n",
      "Fold 2, Epoch 2, Loss: 0.3930619347252344\n",
      "Fold 2, Epoch 3, Loss: 0.363577891598668\n",
      "Fold 2, Epoch 4, Loss: 0.3324931746250705\n",
      "Fold 2, Epoch 5, Loss: 0.2953942751413898\n",
      "Fold 2, Epoch 6, Loss: 0.2673659320724638\n",
      "Fold 2, Epoch 7, Loss: 0.2340891337708423\n",
      "Fold 2, Epoch 8, Loss: 0.19581028819084167\n",
      "Fold 2, Epoch 9, Loss: 0.17148278124238314\n",
      "Fold 2, Epoch 10, Loss: 0.1281999667550911\n",
      "Fold 2, Validation Precision: 0.00%\n",
      "Fold 2, Validation F1 Score: 0.00%\n",
      "Fold 2, Validatoin Confusion Matrix\n",
      " []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Test Precision: 0.80%\n",
      "Fold 2, Test F1 Score 0.81%\n",
      "Fold 2, Test Confusion Matrix\n",
      " [[1475  359]\n",
      " [ 335 1466]]\n",
      "Fold 3, Epoch 1, Loss: 0.3243685670589146\n",
      "Fold 3, Epoch 2, Loss: 0.2416359703791769\n",
      "Fold 3, Epoch 3, Loss: 0.20483815591586263\n",
      "Fold 3, Epoch 4, Loss: 0.16377228531136848\n",
      "Fold 3, Epoch 5, Loss: 0.14189488519179194\n",
      "Fold 3, Epoch 6, Loss: 0.105197360510366\n",
      "Fold 3, Epoch 7, Loss: 0.08822011248322956\n",
      "Fold 3, Epoch 8, Loss: 0.06972953805438521\n",
      "Fold 3, Epoch 9, Loss: 0.059846046282664725\n",
      "Fold 3, Epoch 10, Loss: 0.0647745308703171\n",
      "Fold 3, Validation Precision: 0.00%\n",
      "Fold 3, Validation F1 Score: 0.00%\n",
      "Fold 3, Validatoin Confusion Matrix\n",
      " []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Test Precision: 0.87%\n",
      "Fold 3, Test F1 Score 0.89%\n",
      "Fold 3, Test Confusion Matrix\n",
      " [[1539  255]\n",
      " [ 148 1693]]\n"
     ]
    }
   ],
   "source": [
    "dataset_labels = np.array([data[1] for data in dataset])\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfolds.split(dataset, dataset_labels)):\n",
    "    train_val_subset = Subset(dataset, train_idx)\n",
    "    test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(train_val_subset) - train_size\n",
    "    train_split, val_split = random_split(train_val_subset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_split, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_split, batch_size=64, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    val_precision = precision_score(val_labels, val_preds, average='binary')\n",
    "    val_f1_score = f1_score(val_labels, val_preds, average='binary')\n",
    "    val_confusion_matrix = confusion_matrix(val_labels, val_preds)\n",
    "    print(f\"Fold {fold + 1}, Validation Precision: {val_precision:.2f}%\")\n",
    "    print(f\"Fold {fold + 1}, Validation F1 Score: {val_f1_score:.2f}%\")\n",
    "    print(f\"Fold {fold + 1}, Validatoin Confusion Matrix\\n {val_confusion_matrix}\")\n",
    "\n",
    "\n",
    "    test_labels = []\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    \n",
    "    test_precision = precision_score(test_labels, test_preds, average='binary')\n",
    "    test_f1_score = f1_score(test_labels, test_preds, average='binary')\n",
    "    test_confusion_matrix = confusion_matrix(test_labels, test_preds)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}, Test Precision: {test_precision:.2f}%\")\n",
    "    print(f\"Fold {fold + 1}, Test F1 Score {test_f1_score:.2f}%\")\n",
    "    print(f\"Fold {fold + 1}, Test Confusion Matrix\\n {test_confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def optimized(model, lrates, train_loader, test_loader, criterion, device):\n",
    "    best_f1 = 0\n",
    "    best_precision = 0\n",
    "    best_conf_matrix = None\n",
    "    best_lr = None\n",
    "\n",
    "    for lr in random.sample(lrates, len(lrates)):\n",
    "        print(f\"{lr} currently testing\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(1):\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "        print(f\"Learning rate : {lr}, f1-score: {f1:.2f}, precision: {precision:.2f}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_precision = precision\n",
    "            best_conf_matrix = conf_matrix\n",
    "            best_lr = lr\n",
    "\n",
    "    return best_f1, best_precision, best_conf_matrix, best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 10905\n",
       "    Root location: E:\\Temp\\test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 currently testing\n",
      "Learning rate : 0.01, f1-score: 0.71, precision: 0.71\n",
      "0.001 currently testing\n",
      "Learning rate : 0.001, f1-score: 0.75, precision: 0.75\n",
      "0.0001 currently testing\n",
      "Learning rate : 0.0001, f1-score: 0.75, precision: 0.76\n",
      "0.1 currently testing\n",
      "Learning rate : 0.1, f1-score: 0.34, precision: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "best_f1, best_precision, best_conf_matrix, best_lr = optimized(\n",
    "    model = model,\n",
    "    lrates = [0.0001,0.001,0.01,0.1],\n",
    "    train_loader = train_loader,\n",
    "    test_loader = test_loader,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate: 0.001\n",
      "Best F1-Score: 0.7285218094112712\n",
      "Best Precision: 0.7471600845104602\n",
      "Confusion Matrix:\n",
      "[[671 426]\n",
      " [158 926]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "print(f\"Best F1-Score: {best_f1}\")\n",
    "print(f\"Best Precision: {best_precision}\")\n",
    "print(f\"Confusion Matrix:\\n{best_conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Le_Net5(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fcl1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fcl2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fcl3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model_vgg = models.vgg16(pretrained=True)\n",
    "model_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a train loader and test loader for vgg16 model\n",
    "\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = int(len(dataset) - train_size)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=34, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.classifier[6] = nn.Linear(4096, 2) # 2 is the number of classes we have in our dataset\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_vgg.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model_vgg.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_vgg(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "model_vgg.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model_vgg(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "precision = precision_score(test_labels, test_preds, average='binary')\n",
    "f1 = f1_score(test_labels, test_preds, average='binary')\n",
    "conf_matrix = confusion_matrix(test_labels, test_preds)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matris: \\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
