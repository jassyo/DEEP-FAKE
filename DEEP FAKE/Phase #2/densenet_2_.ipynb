{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dxbSu3bszqm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from google.colab import drive\n",
        "from transformers import ViTForImageClassification\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK5f4BzWtTE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05405e76-c82a-405c-bcc4-2a88e107b09c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU4AMPP1sowq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e84b7b2-a747-463a-d066-13b1fc999c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/test2', transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m3q_Tnvs6GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fd83c0-1ecc-4300-eb59-550fef565f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 81.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained DenseNet121 model\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze all layers except the final classifier layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier layer\n",
        "num_features = model.classifier.in_features\n",
        "model.classifier = torch.nn.Linear(num_features, len(dataset.classes))\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the criterion and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "source": [
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # Get the logits from the ImageClassifierOutput object if using ViT\n",
        "            outputs = model(inputs).logits if isinstance(model, ViTForImageClassification) else model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-sNoUe9BFtAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnbohJ2atFxP",
        "outputId": "29846c12-f151-45b3-bd19-ad4de89f095b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7076, Accuracy: 0.5818\n",
            "Epoch 2/10, Loss: 0.6373, Accuracy: 0.6636\n",
            "Epoch 3/10, Loss: 0.6174, Accuracy: 0.6909\n",
            "Epoch 4/10, Loss: 0.5537, Accuracy: 0.7455\n",
            "Epoch 5/10, Loss: 0.5821, Accuracy: 0.7091\n",
            "Epoch 6/10, Loss: 0.5982, Accuracy: 0.7182\n",
            "Epoch 7/10, Loss: 0.5611, Accuracy: 0.7591\n",
            "Epoch 8/10, Loss: 0.5082, Accuracy: 0.7636\n",
            "Epoch 9/10, Loss: 0.5078, Accuracy: 0.7818\n",
            "Epoch 10/10, Loss: 0.5329, Accuracy: 0.7500\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    dense_net_accuracy  = evaluate_model(model, train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {dense_net_accuracy :.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENAbcYtUtLmk"
      },
      "source": [
        "Step 2: Fine-Tune the Final Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWsH8J-ZtI1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c161e1-1c97-42f2-ed14-13ad97c66e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.7283, Accuracy: 0.6955\n",
            "Epoch 2/10, Loss: 0.6124, Accuracy: 0.6909\n",
            "Epoch 3/10, Loss: 0.5564, Accuracy: 0.8091\n",
            "Epoch 4/10, Loss: 0.5003, Accuracy: 0.8045\n",
            "Epoch 5/10, Loss: 0.4222, Accuracy: 0.8182\n",
            "Epoch 6/10, Loss: 0.4339, Accuracy: 0.8318\n",
            "Epoch 7/10, Loss: 0.3378, Accuracy: 0.9091\n",
            "Epoch 8/10, Loss: 0.3090, Accuracy: 0.9091\n",
            "Epoch 9/10, Loss: 0.2863, Accuracy: 0.9000\n",
            "Epoch 10/10, Loss: 0.2681, Accuracy: 0.9273\n"
          ]
        }
      ],
      "source": [
        "# Unfreeze the last two convolutional layers\n",
        "for param in model.features[6:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Re-define the optimizer to include these layers\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "# Create the new classifier with dropout and move it to the device\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.5),  # Add dropout with a 50% drop rate\n",
        "    torch.nn.Linear(num_features, len(dataset.classes))\n",
        ").to(device) # Move the classifier to the device\n",
        "\n",
        "\n",
        "# Train the model again with fine-tuning\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Evaluate after each epoch\n",
        "    fine_tuned_densenet_accuracy  = evaluate_model(model, train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {fine_tuned_densenet_accuracy :.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luWmYBodtO_I"
      },
      "source": [
        "Step 3: Add One More Convolutional Layer and Fine-Tune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aWcXnd1tPiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fdcaf19-f27e-4f0d-be95-761d9ea62a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3428, Accuracy: 0.9045\n",
            "Epoch 2/10, Loss: 0.2193, Accuracy: 0.9500\n",
            "Epoch 3/10, Loss: 0.1685, Accuracy: 0.9182\n",
            "Epoch 4/10, Loss: 0.1584, Accuracy: 0.9591\n",
            "Epoch 5/10, Loss: 0.1838, Accuracy: 0.9136\n",
            "Epoch 6/10, Loss: 0.2164, Accuracy: 0.9500\n",
            "Epoch 7/10, Loss: 0.1348, Accuracy: 0.9318\n",
            "Epoch 8/10, Loss: 0.1176, Accuracy: 0.9455\n",
            "Epoch 9/10, Loss: 0.1706, Accuracy: 0.9364\n",
            "Epoch 10/10, Loss: 0.1495, Accuracy: 0.9545\n"
          ]
        }
      ],
      "source": [
        "# Add a new convolutional layer\n",
        "model.features.add_module('new_conv', torch.nn.Conv2d(1024, 512, kernel_size=3, padding=1).to(device)) # Move the new layer to the device\n",
        "\n",
        "# Calculate the correct number of in_features for the classifier\n",
        "# This is necessary because we added a new convolutional layer\n",
        "num_features = model.features[-1].out_channels # Get output channels of the last layer\n",
        "\n",
        "# Re-initialize the classifier with the correct in_features\n",
        "model.classifier = torch.nn.Linear(num_features, len(dataset.classes)).to(device)\n",
        "\n",
        "# Unfreeze layers from the new convolution onward\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define optimizer for all layers\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
        "\n",
        "# Train again with the new layer\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Evaluate performance\n",
        "    new_layer_densenet_accuracy  = evaluate_model(model, train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {new_layer_densenet_accuracy :.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Import the os module\n",
        "from PIL import Image #"
      ],
      "metadata": {
        "id": "BAGI4yLWpnT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImOS_FpPtReU"
      },
      "source": [
        "Step 4: Apply Pretrained Vision Transformer (ViT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xYRtMe3tVys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd59acaf-b8a7-47fc-f78b-42eccce79a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 220 image files in /content/drive/MyDrive/test2\n",
            "First 5 files: ['/content/drive/MyDrive/test2/Fake/fake_0.jpg', '/content/drive/MyDrive/test2/Fake/fake_100.jpg', '/content/drive/MyDrive/test2/Fake/fake_10.jpg', '/content/drive/MyDrive/test2/Fake/fake_1.jpg', '/content/drive/MyDrive/test2/Fake/fake_101.jpg']\n",
            "First 5 labels: [0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load the pretrained ViT model\n",
        "vit_model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',  # Provide the model name or path here\n",
        "    hidden_dropout_prob=0.1,  # Specify configuration settings directly\n",
        "    attention_probs_dropout_prob=0.1\n",
        ")\n",
        "vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# Move the ViT model to the appropriate device\n",
        "vit_model = vit_model.to(device)\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_folder, transform=None):\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Get all image files and labels\n",
        "        for class_idx, class_name in enumerate(os.listdir(image_folder)):\n",
        "            class_path = os.path.join(image_folder, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                for file_name in os.listdir(class_path):\n",
        "                    if file_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "                        self.image_files.append(os.path.join(class_path, file_name))\n",
        "                        self.labels.append(class_idx)  # Assign label based on class index\n",
        "\n",
        "        # Print some information for debugging\n",
        "        print(f\"Found {len(self.image_files)} image files in {image_folder}\")\n",
        "        print(f\"First 5 files: {self.image_files[:5]}\")\n",
        "        print(f\"First 5 labels: {self.labels[:5]}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_files[idx]).convert('RGB') # Ensure RGB format\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "vit_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "vit_dataset = CustomDataset('/content/drive/MyDrive/test2', vit_transform)\n",
        "\n",
        "# Create a DataLoader for ViT\n",
        "vit_loader = DataLoader(vit_dataset, batch_size=32, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the optimizer for Vision Transformer\n",
        "optimizer = optim.Adam(vit_model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    vit_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in vit_loader:  # Now get labels from the loader\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit_model(inputs).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Evaluate performance after each epoch\n",
        "    vit_model_accuracy  = evaluate_model(vit_model, vit_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(vit_loader):.4f}, Accuracy: {vit_model_accuracy :.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB6KLfnKFXm2",
        "outputId": "dbd0c7bd-876a-4111-e22b-c7a7d2c052ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6605, Accuracy: 0.8409\n",
            "Epoch 2/10, Loss: 0.4981, Accuracy: 0.8682\n",
            "Epoch 3/10, Loss: 0.3580, Accuracy: 0.9364\n",
            "Epoch 4/10, Loss: 0.2566, Accuracy: 0.9682\n",
            "Epoch 5/10, Loss: 0.1680, Accuracy: 0.9636\n",
            "Epoch 6/10, Loss: 0.1341, Accuracy: 0.9818\n",
            "Epoch 7/10, Loss: 0.1313, Accuracy: 0.9818\n",
            "Epoch 8/10, Loss: 0.0765, Accuracy: 0.9955\n",
            "Epoch 9/10, Loss: 0.0438, Accuracy: 0.9955\n",
            "Epoch 10/10, Loss: 0.0299, Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLYOxOLkta-j"
      },
      "source": [
        "Step 5: Record Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ibS_CD4tXLV"
      },
      "outputs": [],
      "source": [
        "# After training the models, store the results\n",
        "results = {\n",
        "    'DenseNet121': dense_net_accuracy,\n",
        "    'Fine-tuned DenseNet121': fine_tuned_densenet_accuracy,\n",
        "    'DenseNet121 with new layer': new_layer_densenet_accuracy,\n",
        "    'Vision Transformer': vit_model_accuracy\n",
        "}\n",
        "\n",
        "# You can print the results or save them to a file\n",
        "import json\n",
        "with open(\"performance_results.json\", \"w\") as f:\n",
        "    json.dump(results, f)\n"
      ]
    }
  ]
}