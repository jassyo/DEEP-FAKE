{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZWAkuK_iUP7",
        "outputId": "023c20c4-8463-4f08-f689-72e191268530"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g7zfUs2SgxI0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLkJV1UnhmOR",
        "outputId": "ed2bb453-4329-4797-85c4-2391a3f13294"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # InceptionV3 requires input size 299x299\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.ImageFolder(root='/content/drive/MyDrive/test2', transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "RUWZWBL2hnhF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT94ZjyRhqef",
        "outputId": "6ab67edc-cfa5-457a-fbc4-a164efe85661"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to initialize the Inception model\n",
        "def get_model(model_name):\n",
        "    if model_name == \"inception-v3\":\n",
        "        model = models.inception_v3(pretrained=True, aux_logits=True)\n",
        "\n",
        "        # Freeze all layers\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Modify the final fully connected layer for binary classification\n",
        "        model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    return model"
      ],
      "metadata": {
        "id": "1wbfOiVMg2f-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate metrics\n",
        "def calculate_metrics(model, loader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "    total = conf_matrix.sum()\n",
        "    accuracy = (TP + TN) / total if total > 0 else 0.0\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    return accuracy, precision, recall, f1, conf_matrix\n"
      ],
      "metadata": {
        "id": "1gY7c_RGg8xj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5):\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                if isinstance(outputs, tuple):\n",
        "                    outputs = outputs[0]\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "\n",
        "    return best_val_accuracy\n"
      ],
      "metadata": {
        "id": "Ab-G6LVMhAAN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Objective function for hyperparameter optimization\n",
        "def objective(trial, model_name):\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "    model = get_model(model_name).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    val_accuracies = []\n",
        "\n",
        "    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n",
        "        print(f\"Fold {fold_idx + 1}/{num_folds}\")\n",
        "        train_val_data = Subset(dataset, train_val_idx)\n",
        "        test_data = Subset(dataset, test_idx)\n",
        "        train_size = int(0.8 * len(train_val_data))\n",
        "        val_size = len(train_val_data) - train_size\n",
        "        train_data, val_data = torch.utils.data.random_split(\n",
        "            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n",
        "        val_accuracy, _, _, _, _ = calculate_metrics(model, val_loader, device)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "    return np.mean(val_accuracies)\n"
      ],
      "metadata": {
        "id": "yIXjjpCXhCwe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate on the test set\n",
        "def evaluate_test_set(model_name, best_lr):\n",
        "    model = get_model(model_name).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=best_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    fold_metrics = []\n",
        "    for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(dataset)):\n",
        "        print(f\"\\nEvaluating on Fold {fold_idx + 1}/{num_folds}\")\n",
        "        train_val_data = Subset(dataset, train_val_idx)\n",
        "        test_data = Subset(dataset, test_idx)\n",
        "        train_size = int(0.8 * len(train_val_data))\n",
        "        val_size = len(train_val_data) - train_size\n",
        "        train_data, val_data = torch.utils.data.random_split(\n",
        "            train_val_data, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "        train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=5)\n",
        "        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "        fold_metrics.append(calculate_metrics(model, test_loader, device))\n",
        "\n",
        "    avg_accuracy = np.mean([metrics[0] for metrics in fold_metrics])\n",
        "    avg_precision = np.mean([metrics[1] for metrics in fold_metrics])\n",
        "    avg_recall = np.mean([metrics[2] for metrics in fold_metrics])\n",
        "    avg_f1 = np.mean([metrics[3] for metrics in fold_metrics])\n",
        "    total_conf_matrix = np.sum([metrics[4] for metrics in fold_metrics], axis=0)\n",
        "\n",
        "    print(\"\\nAverage Metrics Across Folds:\")\n",
        "    print(f\"Accuracy: {avg_accuracy:.2f}, Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1-Score: {avg_f1:.2f}\")\n",
        "    print(f\"Confusion Matrix (sum of all folds):\\n{total_conf_matrix}\")\n"
      ],
      "metadata": {
        "id": "xhxwUmT-hJy8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_folds = 3\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "DXrVwoklhK-Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in [\"inception-v3\"]:\n",
        "    print(f\"\\nOptimizing for {model_name.upper()}...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(lambda trial: objective(trial, model_name), n_trials=5)\n",
        "    best_lr = study.best_params['lr']\n",
        "    print(f\"Best Learning Rate for {model_name.upper()}: {best_lr}\")\n",
        "    evaluate_test_set(model_name, best_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCl35xd5hOnI",
        "outputId": "06681e07-6dd9-4979-ff7f-78bd2da97348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-29 18:16:25,141] A new study created in memory with name: no-name-4db37a96-defd-4f5a-a10c-7a68b6df265e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimizing for INCEPTION-V3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1652910d8ec9>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/3\n",
            "Epoch 1/5, Loss: 3.3986012786626816\n",
            "Epoch 1/5, Validation Accuracy: 66.67%\n",
            "Epoch 2/5, Loss: 3.140981048345566\n",
            "Epoch 2/5, Validation Accuracy: 33.33%\n",
            "Epoch 3/5, Loss: 3.5508705377578735\n",
            "Epoch 3/5, Validation Accuracy: 63.33%\n",
            "Epoch 4/5, Loss: 1.9418334066867828\n",
            "Epoch 4/5, Validation Accuracy: 70.00%\n",
            "Epoch 5/5, Loss: 1.0411503612995148\n",
            "Epoch 5/5, Validation Accuracy: 43.33%\n",
            "Fold 2/3\n",
            "Epoch 1/5, Loss: 1.1129553020000458\n",
            "Epoch 1/5, Validation Accuracy: 63.33%\n",
            "Epoch 2/5, Loss: 0.9388546496629715\n",
            "Epoch 2/5, Validation Accuracy: 70.00%\n",
            "Epoch 3/5, Loss: 1.049882486462593\n",
            "Epoch 3/5, Validation Accuracy: 80.00%\n",
            "Epoch 4/5, Loss: 0.7679143100976944\n",
            "Epoch 4/5, Validation Accuracy: 76.67%\n",
            "Epoch 5/5, Loss: 0.6571283042430878\n",
            "Epoch 5/5, Validation Accuracy: 73.33%\n",
            "Fold 3/3\n",
            "Epoch 1/5, Loss: 0.879697784781456\n",
            "Epoch 1/5, Validation Accuracy: 76.67%\n",
            "Epoch 2/5, Loss: 1.027362771332264\n",
            "Epoch 2/5, Validation Accuracy: 73.33%\n",
            "Epoch 3/5, Loss: 0.7847142219543457\n",
            "Epoch 3/5, Validation Accuracy: 80.00%\n",
            "Epoch 4/5, Loss: 0.5710965767502785\n",
            "Epoch 4/5, Validation Accuracy: 86.67%\n",
            "Epoch 5/5, Loss: 0.521760918200016\n",
            "Epoch 5/5, Validation Accuracy: 86.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-12-29 18:32:42,972] Trial 0 finished with value: 0.6777777777777777 and parameters: {'lr': 0.014606192555759987}. Best is trial 0 with value: 0.6777777777777777.\n",
            "<ipython-input-13-1652910d8ec9>:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/3\n",
            "Epoch 1/5, Loss: 0.7407654523849487\n",
            "Epoch 1/5, Validation Accuracy: 36.67%\n",
            "Epoch 2/5, Loss: 0.711314469575882\n"
          ]
        }
      ]
    }
  ]
}